
import torch
import torch.nn as nn
from einops import rearrange, repeat, reduce

try:
    from flash_attn.modules.mha import MHA
    FLASH_AVAILABLE = True
except ImportError:
    print("FlashAttention not available, using nn.MultiheadAttention.")
    from torch.nn import MultiheadAttention as MHA
    FLASH_AVAILABLE = False


# Decoder Block
class DecoderBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        
        if FLASH_AVAILABLE:
            self.attn = MHA(d_model, n_heads, dropout=0.1, causal=True)
        else:
            self.attn = nn.MultiheadAttention(
                embed_dim=d_model,
                num_heads=n_heads,
                dropout=0.1,
                batch_first=True,
            )

        self.ln2 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Linear(d_ff, d_model),
        )

    def forward(self, x):
        """
        x: (batch, seq, dim)
        """
        x_norm = self.ln1(x)

        if FLASH_AVAILABLE:
            x = x + self.attn(x_norm)
        else:
            B, T, D = x_norm.shape
            attn_mask = torch.triu(torch.ones(T, T, device=x.device), 1).bool()
            attn_out, _ = self.attn(
                x_norm,
                x_norm,
                x_norm,
                attn_mask=attn_mask,
                need_weights=False,
            )
            x = x + attn_out

        x = x + self.ff(self.ln2(x))
        return x


# Decoder-only LM
class DecoderOnlyLM(nn.Module):
    def __init__(self, vocab_size, n_layers, n_heads, d_model, d_ff, max_seq_len):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Parameter(torch.zeros(1, max_seq_len, d_model))
        self.blocks = nn.ModuleList([
            DecoderBlock(d_model, n_heads, d_ff) for _ in range(n_layers)
        ])
        self.ln_f = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        """
        idx:     (batch, seq)
        targets: (batch, seq), optional
        """
        # print(f"idx shape: {idx.shape}")
        # print(f"targets shape: {targets.shape}")
        B, T = idx.shape

        # token and positional embedding
        tok_emb = self.token_emb(idx)
        pos_emb = self.pos_emb[:, :T, :]
        x = tok_emb + pos_emb

        # transformer blocks
        for block in self.blocks:
            x = block(x)

        # layernorm and head
        x = self.ln_f(x)
        logits = self.lm_head(x)

        if targets is not None:
            logits_flat = rearrange(logits, "b t v -> (b t) v")
            targets_flat = rearrange(targets, "b t -> (b t)")
            loss = nn.functional.cross_entropy(
                logits_flat, targets_flat, ignore_index=-1
            )
            return logits, loss

        return logits


# example usage
if __name__ == "__main__":
    model = DecoderOnlyLM(
        vocab_size=32000,
        n_layers=6,
        n_heads=8,
        d_model=512,
        d_ff=2048,
        max_seq_len=1024,
    )

    idx = torch.randint(0, 32000, (2, 128))
    logits, loss = model(idx, targets=idx)
    print("logits:", logits.shape, "loss:", loss.item())

