# configs/data_prep.yaml

download:
  dataset: wikipedia
  num_tokens: 10000000   # or 300000000, 1B, etc.
  output_path: data/raw/wikipedia_10M.jsonl

paths:
  raw_data: data/raw/wikipedia_10M.jsonl
  deduped_data: data/processed/wikipedia_dedup.jsonl
  removed: data/processed/wikipedia_removed.jsonl
  pretrain_manifest: data/splits/pretrain_manifest.jsonl
  retrieval_manifest: data/splits/retrieval_manifest.jsonl
  stats: data/splits/split_stats.yaml

params:
  total_tokens: 10000000
  pretrain_frac: 0.7
  tokenizer: null
  seed: 42
