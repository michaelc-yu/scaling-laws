import argparse
import hashlib
import io
import json
import re
import sys
from collections import defaultdict
from dataclasses import dataclass
from typing import Iterable, Optional, Tuple, Dict, Any
from urllib.parse import urlparse


_ws_re = re.compile(r"\s+")

def normalize_text(s: str) -> str:
    s = s.replace("\u00A0", " ")
    s = s.replace("\u200b", "")
    s = s.replace("\t", " ")
    s = s.replace("\r", " ")
    s = _ws_re.sub(" ", s).strip()
    return s

# use sha256 for exact dupe

def sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode('utf-8')).hexdigest()


# near-dup with SimHash

import mmh3
import os

WORD_RE = re.compile(r"\w+", re.UNICODE)

def word_ngrams(tokens, n=5):
    for i in range(len(tokens) - n + 1):
        yield tuple(tokens[i:i+n])


def simhash64(text: str, ngram: int = 5) -> int:
    tokens = WORD_RE.findall(text.lower())
    if len(tokens) < ngram:
        # fallback to bag of words simhash
        shingles = [(t,) for t in tokens]
    else:
        shingles = list(word_ngrams(tokens, ngram))
    v = [0] * 64
    for sh in shingles:
        h = mmh3.hash64(" ".join(sh), signed=False)[0]
        for b in range(64):
            v[b] += 1 if (h >> b) & 1 else -1
    fingerprint = 0
    for b in range(64):
        if v[b] > 0:
            fingerprint |= (1 << b)
    return fingerprint


def hamming_distance(x: int, y: int) -> int:
    return (x ^ y).bit_count()


class SimHashIndex:
    def __init__(self, bits_prefix: int = 16):
        self.bits_prefix = bits_prefix
        self.buckets: Dict[int, list[Tuple[int, str]]] = defaultdict(list)

    def _bucket_key(self, fp: int) -> int:
        return fp >> (64 - self.bits_prefix)

    def query_close(self, fp: int, th: int) -> Optional[Tuple[int, str]]:
        bucket = self._bucket_key(fp)
        for cand_fp, doc_id in self.buckets.get(bucket, []):
            if hamming_distance(fp, cand_fp) <= th:
                return cand_fp, doc_id
        return None

    def add(self, fp: int, doc_id: str):
        bucket = self._bucket_key(fp)
        self.buckets[bucket].append((fp, doc_id))


def hostname(url: Optional[str]) -> Optional[str]:
    if not url:
        return None
    try:
        return urlparse(url).hostname
    except Exception:
        return None



def stream_jsonl(path: str) -> Iterable[Dict[str, Any]]:
    with (sys.stdin if path == '-' else open(path, 'r', encoding='utf-8')) as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                yield json.loads(line)
            except json.JSONDecodeError:
                continue


def write_jsonl(path: str, records: Iterable[Dict[str, Any]]):
    with open(path, 'w', encoding='utf-8') as f:
        for rec in records:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")


def main():
    ap = argparse.ArgumentParser(description='Deduplicate corpus with exact + simhash near-dup + optional leakage filters')
    ap.add_argument('--in', dest='inp', required=True, help='Input JSONL corpus')
    ap.add_argument('--out', dest='out', required=True, help='Output deduped JSONL')
    ap.add_argument('--removed', dest='removed', required=True, help='Output JSONL of removed docs with reasons')
    ap.add_argument('--block-domains', default=None, help='Text file with one domain per line to drop (optional)')
    ap.add_argument('--block-hashes', default=None, help='Text file with one sha256 per line to drop (optional)')
    ap.add_argument('--simhash', action='store_true', help='Enable near-dup simhash filtering')
    ap.add_argument('--simhash-th', type=int, default=3, help='Hamming distance threshold (<=3 strong)')
    ap.add_argument('--simhash-prefix', type=int, default=16, help='LSH prefix bits (16-20 recommended)')
    args = ap.parse_args()

    os.makedirs(os.path.dirname(args.out), exist_ok=True)

    blocked_domains = set()
    if args.block_domains:
        with open(args.block_domains, 'r', encoding='utf-8') as f:
            for line in f:
                d = line.strip().lower()
                if d:
                    blocked_domains.add(d)

    blocked_hashes = set()
    if args.block_hashes:
        with open(args.block_hashes, 'r', encoding='utf-8') as f:
            for line in f:
                h = line.strip().lower()
                if h:
                    blocked_hashes.add(h)

    sim_index = SimHashIndex(bits_prefix=args.simhash_prefix) if args.simhash else None

    seen_hashes = set()
    kept = []
    removed = []

    for obj in stream_jsonl(args.inp):
        doc_id = str(obj.get('id') or '')
        url = obj.get('url')
        text = obj.get('text') or ''
        norm = normalize_text(text)
        if not norm:
            removed.append({'id': doc_id, 'url': url, 'reason': 'empty'})
            continue

        # domain block
        host = hostname(url)
        if host and host.lower() in blocked_domains:
            removed.append({'id': doc_id, 'url': url, 'reason': 'blocked_domain'})
            continue

        h = sha256_text(norm)
        if h in blocked_hashes:
            removed.append({'id': doc_id, 'url': url, 'reason': 'blocked_hash'})
            continue
        if h in seen_hashes:
            removed.append({'id': doc_id, 'url': url, 'sha256': h, 'reason': 'exact_dup'})
            continue

        # near-dup
        if sim_index is not None:
            fp = simhash64(norm)
            q = sim_index.query_close(fp, args.simhash_th)
            if q is not None:
                removed.append({'id': doc_id, 'url': url, 'sha256': h, 'reason': 'near_dup_simhash'})
                continue
            sim_index.add(fp, doc_id or h)

        seen_hashes.add(h)
        kept.append({'id': doc_id, 'url': url, 'text': norm, 'sha256': h})

    write_jsonl(args.out, kept)
    write_jsonl(args.removed, removed)

if __name__ == '__main__':
    main()

