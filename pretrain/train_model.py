import argparse
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import os
import yaml

from model.factory import build_model
from pretrain.dataset import build_causal_lm_dataset


MODEL_KEYS = {"n_layers", "n_heads", "d_model", "d_ff", "vocab_size", "max_seq_len"}
TRAIN_KEYS = {"batch_size", "lr", "block_size", "optimizer", "save_dir", "epochs"}
DATA_KEYS  = {"full_data_path", "manifest_path", "tokenizer_name", "num_proc"}


def parse_args():
    parser = argparse.ArgumentParser()

    # YAML config paths (optional)
    parser.add_argument("--model_config", type=str, default="configs/model_sizes.yaml")
    parser.add_argument("--training_config", type=str, default="configs/training.yaml")
    parser.add_argument("--data_config", type=str, default="configs/data.yaml")
    parser.add_argument("--size", type=str, default="tiny")

    # Direct override args
    # Model params
    parser.add_argument("--n_layers", type=int)
    parser.add_argument("--n_heads", type=int)
    parser.add_argument("--d_model", type=int)
    parser.add_argument("--d_ff", type=int)
    parser.add_argument("--vocab_size", type=int)
    parser.add_argument("--max_seq_len", type=int)

    # Training params
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--block_size", type=int)
    parser.add_argument("--optimizer", type=str)
    parser.add_argument("--save_dir", type=str)
    parser.add_argument("--epochs", type=int)

    # Data params
    parser.add_argument("--full_data_path", type=str)
    parser.add_argument("--manifest_path", type=str)
    parser.add_argument("--tokenizer_name", type=str)
    parser.add_argument("--num_proc", type=int)

    return parser.parse_args()


def maybe_override(cfg: dict, args: argparse.Namespace, key: str):
    """Override YAML config value with CLI arg if provided."""
    val = getattr(args, key, None)
    if val is not None:
        cfg[key] = val
    return cfg


def load_configs(args):
    # Load YAML or empty dict
    model_cfg = yaml.safe_load(open(args.model_config))["models"][args.size] if args.model_config else {}
    train_cfg = yaml.safe_load(open(args.training_config))["training"] if args.training_config else {}
    data_cfg  = yaml.safe_load(open(args.data_config))["data"] if args.data_config else {}

    # Apply CLI overrides only to the right dict
    for key, val in vars(args).items():
        if val is None:
            continue

        if key in MODEL_KEYS:
            model_cfg[key] = val
        elif key in TRAIN_KEYS:
            train_cfg[key] = val
        elif key in DATA_KEYS:
            data_cfg[key] = val

    return model_cfg, train_cfg, data_cfg


def train_single_model(model_cfg, train_cfg, data_cfg, device):
    """Train one single LM model."""

    lr = float(train_cfg["lr"])
    block_sz = train_cfg["block_size"]
    batch_sz = train_cfg["batch_size"]
    save_dir = train_cfg["save_dir"]
    num_epochs = train_cfg["epochs"]

    full_data_path = data_cfg["full_data_path"]
    manifest_path = data_cfg["manifest_path"]
    tokenizer_name = data_cfg["tokenizer_name"]
    num_proc = data_cfg.get("num_proc", None)

    print(f"[DBG] Building model...")
    print("=" * 30)
    model = build_model(model_cfg).to(device)
    print(f"[DBG] Model: {model}")
    print("=" * 30)

    optimizer = AdamW(model.parameters(), lr=lr)

    print("[DBG] Loading dataset...")
    print("=" * 30)
    train_ds, tokenizer = build_causal_lm_dataset(
        full_data_path, manifest_path, tokenizer_name, block_sz, num_proc
    )

    dataloader = DataLoader(train_ds, batch_sz, shuffle=True)

    os.makedirs(save_dir, exist_ok=True)
    model.train()

    print("[DBG] Starting training...")

    for epoch in range(num_epochs):
        total_loss = 0
        for batch in tqdm(dataloader):
            input_ids = batch["input_ids"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            _, loss = model(input_ids, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"[DBG] Epoch {epoch+1} | Loss {avg_loss:.4f}")

        torch.save(model.state_dict(), f"{save_dir}/epoch{epoch+1}.pt")


def main():
    args = parse_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"

    model_cfg, train_cfg, data_cfg = load_configs(args)

    print("=" * 30)
    print(f"[DBG] model cfg: {model_cfg}")
    print("=" * 30)
    print(f"[DBG] train cfg: {train_cfg}")
    print("=" * 30)
    print(f"[DBG] data cfg: {data_cfg}")
    print("=" * 30)

    train_single_model(model_cfg, train_cfg, data_cfg, device)


if __name__ == "__main__":
    main()
