# pretrain/dataset.py
import json
from typing import Optional
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer
import numpy as np

def _load_and_filter(full_data_path: str, manifest_path: str) -> Dataset:
    dataset = load_dataset("json", data_files=full_data_path, split="train")
    with open(manifest_path, "r") as f:
        ids = {json.loads(line)["id"] for line in f}
    dataset = dataset.filter(lambda x: x["id"] in ids)
    # keep only the text column
    drop = [c for c in dataset.column_names if c != "text"]
    if drop:
        dataset = dataset.remove_columns(drop)
    return dataset

def _tokenize_concat_chunk(
    ds: Dataset,
    tokenizer: AutoTokenizer,
    block_size: int,
    num_proc: Optional[int] = None,
) -> Dataset:
    # print(f"1 ds: {ds}")
    # tokenize each example (no truncation — we’ll pack)
    def tok_fn(batch):
        out = tokenizer(batch["text"], add_special_tokens=False)
        return {"input_ids": out["input_ids"]}

    ds = ds.map(tok_fn, batched=True, remove_columns=["text"], num_proc=num_proc)
    # print(f"2 ds: {ds}")

    # concatenate all ids then split into fixed blocks of length (block_size + 1)
    # (+1 so we can create shifted labels for next-token prediction)
    def pack_fn(examples):
        # flatten
        all_ids = [ids for seq in examples["input_ids"] for ids in seq]

        seq_len = block_size + 1
        n_full = len(all_ids) // seq_len
        if n_full == 0:
            return {"input_ids": [], "labels": []}

        arr = np.array(all_ids[: n_full * seq_len], dtype=np.int64)
        arr = arr.reshape(n_full, seq_len)

        # inputs: first block_size tokens; labels: next token for each position
        inputs = arr[:, :-1]
        labels = arr[:, 1:]

        return {
            "input_ids": [row.tolist() for row in inputs],
            "labels": [row.tolist() for row in labels],
        }

    ds = ds.map(
        pack_fn,
        batched=True,
        batch_size=1000,
        remove_columns=["input_ids"],
    )

    # print(f"3 ds: {ds}")

    # Set tensor format for PyTorch
    ds.set_format(type="torch", columns=["input_ids", "labels"])

    # print(f"4 ds: {ds}")
    return ds

def build_causal_lm_dataset(
    full_data_path: str,
    manifest_path: str,
    tokenizer_name: str,
    block_size: int,
    num_proc: Optional[int] = None,
) -> tuple[Dataset, AutoTokenizer]:
    """
    Returns a HF Dataset with columns: input_ids (L), labels (L) where labels are next-token targets.
    Ensures tokenizer has a pad token (set to eos if missing).
    """
    ds = _load_and_filter(full_data_path, manifest_path)
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)
    tokenizer.pad_token = tokenizer.eos_token

    ds = _tokenize_concat_chunk(ds, tokenizer, block_size=block_size, num_proc=num_proc)

    # input tokens: a sequence of token IDs from the tokenizer
    # target tokens: the same sequence but shifted one token to the left
    return ds, tokenizer
